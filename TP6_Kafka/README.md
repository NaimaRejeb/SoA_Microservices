# TP6 - IntÃ©gration et Manipulation de DonnÃ©es avec Kafka


**MatiÃ¨re** : SoA & Microservices  
**Enseignant** : Dr. Salah Gontara  
**Classe** : 4Info  DataScience & AI 

**Auteur** : Naima REJEB

---

## ğŸ“Œ Objectifs
- AcquÃ©rir des compÃ©tences pratiques dans la gestion des flux de donnÃ©es avec Apache Kafka
- Apprendre Ã  intÃ©grer Kafka avec des applications Node.js pour la production et la consommation de messages
- Persister les donnÃ©es dans MongoDB
- Exposer les donnÃ©es via une API REST
- Tester lâ€™intÃ©gration complÃ¨te avec MongoDB Atlas et Postman

---
## ğŸ› ï¸ PrÃ©requis

- [Node.js](https://nodejs.org/)
- [Apache Kafka](https://kafka.apache.org/downloads) (v3.9.0)
- [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)
- [Postman](https://www.postman.com/)

---

## âš™ï¸ Outils UtilisÃ©s
- Kafka
- Zookeeper
- KafkaJS
- MongoDB Atlas
- Express.js
- Postman
---

## ğŸ“ Ã‰tapes du TP  

## ğŸ“¥ Configuration du TP  

#### Ã‰tape 1 : TÃ©lÃ©chargement

- Installer Node.js depuis : https://nodejs.org/en/download  
- TÃ©lÃ©charger Kafka (v3.9.0) depuis : https://kafka.apache.org/downloads
  
#### Ã‰tape 2 : DÃ©marrer le serveur Zookeeper

```bash
bin/zookeeper-server-start.sh config/zookeeper.properties
```

#### Ã‰tape 3 : DÃ©marrer le serveur Kafka

```bash
bin/kafka-server-start.sh config/server.properties
```

#### Ã‰tape 4 : CrÃ©er un Topic Kafka

```bash
bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --topic test-topic --bootstrap-server localhost:9092
```

## ğŸ§± Installation des DÃ©pendances

```bash
npm init -y
npm install kafkajs express mongoose
```

## âš™ï¸ Travail Ã  faire 

## ğŸ“¨ Production de Messages

*producer.js*

```bash
const { Kafka } = require('kafkajs');
const kafka = new Kafka({
clientId: 'my-app',
brokers: ['localhost:9092']
});
const producer = kafka.producer();
const run = async () => {
await producer.connect();
setInterval(async () => {
try {
await producer.send({
topic: 'test-topic',
messages: [
{ value: 'Hello KafkaJS user!' },
],
});
console.log('Message produit avec succÃ¨s');
} catch (err) {
console.error("Erreur lors de la production de message", err);
}
}, 1000);
};
run().catch(console.error);
```
## ğŸ“¥ Consommation de Messages + MongoDB

*consumer.js*

```bash
    const { Kafka } = require('kafkajs');
    const { MongoClient } = require('mongodb');

    // Get this EXACT string from MongoDB Atlas interface
    const mongoUrl = 'mongodb+srv://kafka-naima:kafka_naima@cluster0.lxg3tpl.mongodb.net/Kafka?retryWrites=true&w=majority';

    const kafka = new Kafka({
    clientId: 'my-app',
    brokers: ['localhost:9092']
    });
    const run = async () => {
    const client = await MongoClient.connect(mongoUrl);
    const db = client.db('Kafka');
    const collection = db.collection('kafka_messages');

    const consumer = kafka.consumer({ groupId: 'test-group' });
    await consumer.connect();
    await consumer.subscribe({ topic: 'test-topic', fromBeginning: true });

    await consumer.run({
        eachMessage: async ({ topic, partition, message }) => {
        try {
            await collection.insertOne({
            message: message.value.toString(),
            topic,
            partition,
            timestamp: new Date()
            });
            console.log('Saved to MongoDB');
        } catch (err) {
            console.error('MongoDB Error:', err);
        }
        },
    });
    };

run().catch(err => {
    console.error('Fatal error:', err);
    process.exit(1);
});
```
## ğŸŒ CrÃ©ation de lâ€™API REST

*api.js*

```bash
const express = require('express');
const { MongoClient } = require('mongodb');

const app = express();
const port = 3000;
const mongoUrl = 'mongodb+srv://kafka-naima:kafka_naima@cluster0.lxg3tpl.mongodb.net/Kafka?retryWrites=true&w=majority';
const dbName = 'Kafka'; // â† ModifiÃ© pour Ãªtre cohÃ©rent (majuscule)
const collectionName = 'kafka_messages';

app.get('/messages', async (req, res) => {
const client = new MongoClient(mongoUrl);
try {
    await client.connect();
    const db = client.db(dbName); // â† Utilise la variable cohÃ©rente
    const messages = await db.collection(collectionName).find().toArray();
    res.json(messages);
} catch (err) {
    console.error("Erreur MongoDB:", err);
    res.status(500).json({ error: "Database error" });
} finally {
    await client.close(); // â† Fermeture garantie
}
});

app.listen(port, () => {
console.log(`API disponible sur http://localhost:${port}`);
});
```
## ğŸ§ª Tests

#### 1. â–¶ï¸ Lancer les services

# Terminal 1
```bash
bin/zookeeper-server-start.sh config/zookeeper.properties
```
# Terminal 2
```bash
bin/kafka-server-start.sh config/server.properties
```
# Terminal 3
```bash
node producer.js
```
![Producer ](test_screenshots/producer.png)
# Terminal 4
```bash
node consumer.js
```
![Consumer ](test_screenshots/consumer.png)
# Terminal 5
```bash
node api.js
```
![API ](test_screenshots/api.png)

#### 2. ğŸ“« Tester lâ€™API avec Postman
- MÃ©thode : GET
- URL : http://localhost:3000/messages
- 
![Test avec Postman ](test_screenshots/postman_test.png)

---
#### 3. ğŸ’¾ Tester avec MongoDB Atlas
1. Connexion au compte MongoDB Atlas.

2. Cluster => "Browse Collections".

3. SÃ©lection de la base de donnÃ©es et la collection utilisÃ©es(dans mon cas: Kafka, kafka_messages).

![Test avec MongoDB_Atlas ](test_screenshots/mongodb.png)


### ğŸ“‚ Structure du Projet

```plaintext
             
TP6_Kafka_Node/
â”œâ”€â”€ node_modules/         # ğŸ“¦ DÃ©pendances
â”œâ”€â”€ test_screenshots/     # ğŸ“œ Les captures des tests effectuÃ©s 
â”œâ”€â”€ producer.js           # ğŸš€ Producteur Kafka
â”œâ”€â”€ consumer.js           # ğŸ“¥ Consommateur Kafka + MongoDB
â”œâ”€â”€ api.js                # ğŸŒ API REST
â”œâ”€â”€ package.json          # âš™ï¸ Config projet
â””â”€â”€ README.md             # ğŸ“– Documentation



```